# core/question_generator.py
from __future__ import annotations

import json
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

from core.llm_text import call_llm_text

# =========================
# Config
# =========================

@dataclass
class QuestionGenConfig:
    model: str = "gpt-4o-mini"
    temperature: float = 0.3
    max_chunks: int = 40
    chunk_chars: int = 450
    preview_chars: int = 120


# =========================
# Text â†’ Evidence chunks
# =========================

_PAGE_RE = re.compile(r"-{5}\s*PAGE\s+(\d+)\s*-{5}", re.IGNORECASE)


def _split_text_to_chunks(text: str, cfg: QuestionGenConfig) -> List[Dict[str, Any]]:
    """PAGE ë‹¨ìœ„ â†’ chunk ë‹¨ìœ„ ë¶„í•´"""
    chunks: List[Dict[str, Any]] = []
    text = (text or "").strip()
    if not text:
        return []

    matches = list(_PAGE_RE.finditer(text))
    pages: List[tuple[int, str]] = []

    if not matches:
        pages = [(0, text)]
    else:
        for i, m in enumerate(matches):
            start = m.end()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            page_no = int(m.group(1))
            pages.append((page_no, text[start:end].strip()))

    for page_no, page_text in pages:
        if not page_text:
            continue

        for idx in range(0, len(page_text), cfg.chunk_chars):
            body = page_text[idx: idx + cfg.chunk_chars].strip()
            if not body:
                continue

            chunk_id = f"p{page_no}_c{idx // cfg.chunk_chars:02d}"
            preview = body.replace("\n", " ")[: cfg.preview_chars]

            chunks.append({
                "kind": "text",
                "page": page_no,
                "chunk_id": chunk_id,
                "preview": preview,
            })

            if len(chunks) >= cfg.max_chunks:
                return chunks

    return chunks


# =========================
# Tables normalize
# =========================

def _normalize_tables_format(tables: Any) -> List[Dict[str, Any]]:
    """í‘œ í¬ë§· í‘œì¤€í™”"""
    if not isinstance(tables, list):
        return []

    out: List[Dict[str, Any]] = []
    for t in tables:
        if not isinstance(t, dict):
            continue

        page = t.get("page")
        if not isinstance(page, int):
            page = t.get("page_index") or t.get("page_number")
        if not isinstance(page, int):
            page = None

        headers = t.get("headers") or t.get("columns") or t.get("cols")
        if not isinstance(headers, list):
            headers = []

        rows = t.get("rows") or t.get("data") or t.get("values") or t.get("cells")
        if not isinstance(rows, list):
            rows = []

        norm_rows: List[List[Any]] = []
        for r in rows:
            if isinstance(r, list):
                norm_rows.append(r)
            elif isinstance(r, dict):
                if headers:
                    norm_rows.append([r.get(h) for h in headers])
                else:
                    norm_rows.append(list(r.values()))
            else:
                norm_rows.append([r])

        content = t.get("content")
        if isinstance(content, str) and content.strip():
            content = content.strip()
        else:
            content = None

        source = t.get("source") if isinstance(t.get("source"), str) else None

        out.append({
            "page": page,
            "headers": headers,
            "rows": norm_rows,
            "content": content,
            "source": source,
        })

    return out


# =========================
# Prompt
# =========================

def _truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[:n] + " ...[truncated]"


def _compute_type_distribution(qn: int, types_ratio: Optional[Dict[str, float]]) -> Dict[str, int]:
    """
    types_ratioì— ë”°ë¼ MCQ/SAQ ë¬¸ì œ ìˆ˜ ê³„ì‚°

    Args:
        qn: ì´ ë¬¸ì œ ìˆ˜
        types_ratio: {"MCQ": 0.7, "SAQ": 0.3} í˜•íƒœ

    Returns:
        {"MCQ": n, "SAQ": m}
    """
    if not types_ratio or not isinstance(types_ratio, dict):
        # ê¸°ë³¸ê°’: ì „ë¶€ MCQ
        return {"MCQ": qn, "SAQ": 0}

    mcq_ratio = float(types_ratio.get("MCQ", 1.0))
    saq_ratio = float(types_ratio.get("SAQ", 0.0))

    # ë¹„ìœ¨ ì •ê·œí™”
    total_ratio = mcq_ratio + saq_ratio
    if total_ratio <= 0:
        return {"MCQ": qn, "SAQ": 0}

    mcq_ratio /= total_ratio
    saq_ratio /= total_ratio

    n_mcq = round(qn * mcq_ratio)
    n_saq = qn - n_mcq

    return {"MCQ": max(0, n_mcq), "SAQ": max(0, n_saq)}


def _compute_difficulty_distribution(
    qn: int,
    difficulty: str,
) -> Dict[str, int]:
    """
    difficulty ì„¤ì •ì— ë”°ë¼ ë‚œì´ë„ë³„ ë¬¸ì œ ìˆ˜ ê³„ì‚°

    Args:
        qn: ì´ ë¬¸ì œ ìˆ˜
        difficulty: "easy" | "medium" | "hard" | "mixed"

    Returns:
        {"easy": n, "medium": m, "hard": k}
    """
    difficulty = (difficulty or "mixed").strip().lower()

    if difficulty == "easy":
        return {"easy": qn, "medium": 0, "hard": 0}
    elif difficulty == "medium":
        return {"easy": 0, "medium": qn, "hard": 0}
    elif difficulty == "hard":
        return {"easy": 0, "medium": 0, "hard": qn}
    else:  # mixed
        # ê· ë“± ë¶„ë°°: easy:2, medium:2, hard:1 (5ë¬¸ì œ ê¸°ì¤€)
        n_easy = max(1, qn // 4)
        n_hard = max(1, qn // 4)
        n_medium = qn - n_easy - n_hard
        return {"easy": n_easy, "medium": n_medium, "hard": n_hard}


def _build_prompt(
    job: Dict[str, Any],
    chunks: List[Dict[str, Any]],
    tables: Optional[List[Dict[str, Any]]] = None,
) -> str:
    qn = int(job.get("target_questions") or 0) or 2
    section_id = job.get("section_id", "unknown")

    chunk_lines = [
        f"- {c.get('chunk_id')} (page {c.get('page')}): {c.get('preview','')}"
        for c in chunks
    ]

    tables = tables or []
    num_tables = len(tables)
    has_tables = num_tables > 0

    constraints = job.get("constraints") if isinstance(job.get("constraints"), dict) else {}
    must_use_tables = bool(constraints.get("must_use_tables")) or bool(constraints.get("has_tables_in_job"))

    # types_ratioì— ë”°ë¥¸ MCQ/SAQ ë¶„í¬ ê³„ì‚°
    types_ratio = job.get("types_ratio")
    type_dist = _compute_type_distribution(qn, types_ratio)
    n_mcq = type_dist["MCQ"]
    n_saq = type_dist["SAQ"]

    # difficultyì— ë”°ë¥¸ ë‚œì´ë„ ë¶„í¬ ê³„ì‚°
    difficulty_setting = job.get("difficulty", "mixed")
    diff_dist = _compute_difficulty_distribution(qn, difficulty_setting)
    n_easy = diff_dist["easy"]
    n_medium = diff_dist["medium"]
    n_hard = diff_dist["hard"]

    # í‘œ ê¸°ë°˜ ë¬¸ì œ ê°œìˆ˜(í‘œê°€ ìˆìœ¼ë©´ ìµœì†Œ ì ˆë°˜)
    if has_tables:
        n_table_questions = max(1, (qn + 1) // 2)
    else:
        n_table_questions = 0

    # í‘œ ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    table_section = ""
    if has_tables:
        table_section = "\n" + "=" * 70 + "\nğŸ“Š ì¶”ì¶œëœ í‘œ ë°ì´í„°(ì›ë¬¸ì—ì„œ íŒŒì‹±ë¨)\n" + "=" * 70 + "\n"
        for i, tbl in enumerate(tables, 1):
            page = tbl.get("page")
            headers = tbl.get("headers") or []
            rows = tbl.get("rows") or []
            content = tbl.get("content")

            snippet = ""
            if isinstance(content, str) and content.strip():
                snippet = _truncate(content.strip(), 1500)
            else:
                try:
                    sample = {
                        "headers": headers[:12],
                        "rows_sample": rows[:8],
                        "total_rows": len(rows),
                    }
                    snippet = _truncate(json.dumps(sample, ensure_ascii=False, indent=2), 1500)
                except Exception:
                    snippet = ""

            table_section += f"\n[í‘œ {i}] (í˜ì´ì§€ {page})\n{snippet}\n"

    # í‘œ ì§€ì‹œë¬¸(êµìˆ˜ ìŠ¤íƒ€ì¼ë¡œ ê°•í™”)
    if has_tables:
        table_instruction = f"""
{"=" * 70}
ğŸ“Š í‘œ ê¸°ë°˜ ë¬¸í•­ ì¶œì œ ê·œì¹™ (í‘œê°€ ìˆëŠ” ì„¹ì…˜ì´ë©´ í•„ìˆ˜)
{"=" * 70}

- ì´ {qn}ë¬¸í•­ ì¤‘ **ìµœì†Œ {n_table_questions}ë¬¸í•­ì€ ë°˜ë“œì‹œ í‘œ ê¸°ë°˜**ìœ¼ë¡œ ì¶œì œ.
- í‘œ ê¸°ë°˜ ë¬¸í•­ì€ JSONì— "generated_table" í•„ë“œë¥¼ **ë°˜ë“œì‹œ í¬í•¨**.
- "generated_table"ì€ ì›ë³¸ í‘œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³ , **êµ¬ì¡°ëŠ” ì°¸ê³ í•˜ë˜ ìˆ˜ì¹˜/ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒˆë¡œ êµ¬ì„±**.
- í‘œëŠ” ë°˜ë“œì‹œ "í’€ì´ ê°€ëŠ¥"í•´ì•¼ í•˜ë©°, ì™¸ë¶€ ì§€ì‹ ì—†ì´ í‘œë§Œìœ¼ë¡œ í’€ë ¤ì•¼ í•¨.

[generated_table í’ˆì§ˆ ì¡°ê±´]
- headersëŠ” ì˜ë¯¸/ë‹¨ìœ„ê°€ ë“œëŸ¬ë‚˜ì•¼ í•¨ (ì˜ˆ: "ì§€ì—°(ms)", "ì •í™•ë„(%)", "ë¹„ìš©(ì›)")
- í–‰ 3~8ê°œ, ì—´ 2~5ê°œ ê¶Œì¥
- ìˆ˜ì¹˜ ì—´ ìµœì†Œ 2ê°œ í¬í•¨ (ë¹„êµ/ê³„ì‚° ê°€ëŠ¥í•˜ë„ë¡)
- ê³„ì‚°/ë¹„ìœ¨/íš¨ìœ¨ ë¬¸ì œëŠ” explanationì— **ì¤‘ê°„ ê³„ì‚° 1ì¤„** í¬í•¨

[í‘œ ê¸°ë°˜ ë¬¸í•­ ìœ í˜•(ë‹¤ì–‘í•˜ê²Œ ì„ê¸°)]
1) ë¹„êµ/ìš°ì„ ìˆœìœ„: ìµœëŒ€/ìµœì†Œ/ì°¨ì´/ê°œì„ í­
2) ê³„ì‚°/ë¹„ìœ¨/ì¦ê°ë¥ /íš¨ìœ¨: í‰ê· , ë¹„ìœ¨, ì¦ê°€ìœ¨, ì„±ëŠ¥/ì‹œê°„ ë“±
3) ì¡°ê±´ë¶€ ì¶”ì¶œ: ì„ê³„ê°’, ì¡°ê±´ ì¶©ì¡± í–‰ ê°œìˆ˜, í•„í„°ë§ ê²°ê³¼
4) íŒ¨í„´/ì¶”ì„¸: ì‹œê°„/ë²„ì „/ì‹¤í—˜ì¡°ê±´ ë³€í™”ì— ë”°ë¥¸ ê²½í–¥
5) í•´ì„/ê²°ë¡ : ë°ì´í„°ë¡œë¶€í„° íƒ€ë‹¹í•œ ê²°ë¡  ì„ íƒ

[ê¸ˆì§€]
- ë‹¨ìˆœ ê°’ ì°¾ê¸°("Aì˜ ê°’ì€?") ìˆ˜ì¤€ì˜ ë¬¸í•­
- í‘œ ì—†ì´ë„ í’€ ìˆ˜ ìˆëŠ” ë¬¸í•­
- ì›ë³¸ í‘œ ê·¸ëŒ€ë¡œ ë³µë¶™
"""
    else:
        if must_use_tables:
            # í‘œê°€ ìˆì–´ì•¼ í•˜ëŠ”ë° ì—†ëŠ” ê²½ìš°: ëª¨ë¸ì´ í—›ì†Œë¦¬ í‘œë¥¼ ë§Œë“¤ì§€ ì•Šë„ë¡ ê²½ê³ ë¥¼ ë„£ìŒ
            table_instruction = f"""
{"=" * 70}
âš ï¸ í‘œ ì‚¬ìš© ìš”êµ¬ë¨(ì¤‘ìš”) / í•˜ì§€ë§Œ í˜„ì¬ ì…ë ¥ tablesê°€ ë¹„ì–´ìˆìŒ
{"=" * 70}
- ì´ ì„¹ì…˜ì€ í‘œ ê¸°ë°˜ ë¬¸í•­ì´ ìš”êµ¬ë˜ì§€ë§Œ, ì œê³µëœ í‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.
- **ì„ì˜ë¡œ í‘œë¥¼ ê¾¸ë©°ë‚´ì§€ ë§ˆì„¸ìš”.**
- í‘œê°€ ì—†ìœ¼ë¯€ë¡œ ì´ë²ˆ ì¶œë ¥ì—ì„œëŠ” "generated_table"ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""
        else:
            table_instruction = """
**[í‘œ ì—†ìŒ]** ì´ ì„¹ì…˜ì—ëŠ” í‘œê°€ ì—†ìœ¼ë¯€ë¡œ "generated_table" í•„ë“œëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""

    # í•µì‹¬: êµìˆ˜ê¸‰ í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸(LLMì´ ìŠ¤ìŠ¤ë¡œ ì¤€ìˆ˜í•˜ë„ë¡ ê°•ì œ)
    quality_block = f"""
{"=" * 70}
ğŸ§‘â€ğŸ« êµìˆ˜ ì¶œì œ ì›ì¹™ (íƒ€ë‹¹ë„/ë³€ë³„ë„/ì±„ì ê°€ëŠ¥ì„±)
{"=" * 70}

[1] ê·¼ê±° ì •í•©ì„± (ìµœìš°ì„ )
- ê° ë¬¸í•­ì€ evidenceë¥¼ **1~2ê°œë§Œ** ì‚¬ìš©.
- ì •ë‹µ/í•´ì„¤ì€ evidence ì²­í¬ì˜ ë‚´ìš©(ì •ì˜/ê´€ê³„/ì ˆì°¨/ê²°ë¡ )ì—ì„œë§Œ ë„ì¶œ.
- ê·¼ê±° ì—†ì´ ì™¸ë¶€ ì§€ì‹ìœ¼ë¡œë§Œ í’€ë¦¬ëŠ” ë¬¸í•­ ê¸ˆì§€.

[2] ë‹¨ì¼ ì •ë‹µì„± (MCQ)
- ì •ë‹µì€ **ì˜¤ì§ 1ê°œ**ì—¬ì•¼ í•¨. ë³µìˆ˜ ì •ë‹µ ê°€ëŠ¥ì„±ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆìœ¼ë©´ ë¬¸í•­ì„ ë‹¤ì‹œ ì‘ì„±.
- ì§ˆë¬¸ ë¬¸ì¥ì— ì¡°ê±´/ë²”ìœ„ë¥¼ ëª…í™•íˆ í¬í•¨ (ì˜ˆ: íŠ¹ì • ìƒí™©, ì „ì œ, ê¸°ì¤€).

[3] ì˜¤ë‹µ ì„¤ê³„ (ë³€ë³„ë„)
- ì˜¤ë‹µì€ "í”í•œ ì˜¤ê°œë…/ìœ ì‚¬ê°œë… í˜¼ë™/ê²½ê³„ì¡°ê±´ ì°©ê°/ë¶€ë¶„ì ìœ¼ë¡œë§Œ ë§ëŠ” ì§„ìˆ " ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„.
- ì •ë‹µê³¼ ì˜¤ë‹µì˜ ê¸¸ì´/í˜•íƒœ ìœ ì‚¬í•˜ê²Œ.
- ë¬´ê´€í•œ ì˜¤ë‹µ/ë„ˆë¬´ ìëª…í•œ ì˜¤ë‹µ ê¸ˆì§€.

[4] ë‚œì´ë„ ì •ì˜ (ì¡°ì‘ì ìœ¼ë¡œ ì¤€ìˆ˜)
- easy: ì •ì˜/ìš©ì–´/í•µì‹¬ ë¬¸ì¥ í™•ì¸ (ì¶”ë¡  0~1 step)
- medium: ì‘ì€ ìƒí™© ì ìš©/ë¹„êµ/ê°„ë‹¨ ê³„ì‚° (ì¶”ë¡  1~2 step)
- hard: ê²½ê³„ì¡°ê±´/ë°˜ë¡€/ë³µí•© ì¶”ë¡ /íŠ¸ë ˆì´ë“œì˜¤í”„/ë‹¤ë‹¨ê³„ ê³„ì‚° (ì¶”ë¡  3 step ì´ìƒ)

[5] í•´ì„¤ ê·œì¹™
- explanationì€ **2~4ë¬¸ì¥**.
- ë°˜ë“œì‹œ "ì™œ ì •ë‹µì¸ì§€" + "ëŒ€í‘œ ì˜¤ë‹µ 1ê°œê°€ ì™œ í‹€ë ¸ëŠ”ì§€" í¬í•¨.
- ê³„ì‚°í˜•ì€ ì¤‘ê°„ ê³„ì‚° 1ì¤„ í¬í•¨.

[6] ë¬¸í•­ ë‹¤ì–‘ì„±
- ë™ì¼í•œ ì§ˆë¬¸ íŒ¨í„´ 2íšŒ ì´ìƒ ë°˜ë³µ ê¸ˆì§€.
- ê°€ëŠ¥í•˜ë©´ ë¹„êµ/ë¶„ì„í˜• â‰¥1, ì ìš©í˜• â‰¥1 í¬í•¨.
"""

    # ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ê°•í™”: API ëª…ì„¸ í•„ë“œëª… ì‚¬ìš©
    output_schema = """
{
  "questions": [
    {
      "question_id": "Q001",
      "type": "MCQ",
      "difficulty": "easy",
      "question_text": "ë¬¸ì œ ë‚´ìš©",
      "options": ["A) ...", "B) ...", "C) ...", "D) ..."],
      "correct_answer": "B",
      "explanation": "2~4ë¬¸ì¥ í•´ì„¤(ì •ë‹µ ê·¼ê±° + ëŒ€í‘œ ì˜¤ë‹µ ë°˜ë°• í¬í•¨).",
      "source_pages": [5, 6],
      "evidence": [{"kind": "text", "page": 5, "chunk_id": "p5_c00"}],

      "learning_objective": "ì´ ë¬¸í•­ì´ í‰ê°€í•˜ëŠ” í•™ìŠµ ëª©í‘œ(í•œ ì¤„)",
      "common_misconception": "í•™ìƒì´ ìì£¼ í•˜ëŠ” ì˜¤ê°œë…(í•œ ì¤„)",

      "generated_table": {
        "headers": ["ì—´1", "ì—´2"],
        "rows": [["ê°’1","ê°’2"], ["ê°’3","ê°’4"]]
      },
      "table_refs": ["table_5_1"]
    }
  ]
}
""".strip()

    # ìœ í˜• êµ¬ì„± í…ìŠ¤íŠ¸
    type_composition = f"MCQ(ê°ê´€ì‹) {n_mcq}ê°œ"
    if n_saq > 0:
        type_composition += f" / SAQ(ë‹¨ë‹µí˜•) {n_saq}ê°œ"

    # ë‚œì´ë„ êµ¬ì„± í…ìŠ¤íŠ¸
    diff_parts = []
    if n_easy > 0:
        diff_parts.append(f"easy {n_easy}ê°œ")
    if n_medium > 0:
        diff_parts.append(f"medium {n_medium}ê°œ")
    if n_hard > 0:
        diff_parts.append(f"hard {n_hard}ê°œ")
    diff_composition = " / ".join(diff_parts) if diff_parts else "mixed"

    return f"""
ë‹¹ì‹ ì€ ì†Œí”„íŠ¸ì›¨ì–´í•™ë¶€ ëŒ€í•™ êµìˆ˜ë¡œì„œ, ì„¹ì…˜ "{section_id}"ì— ëŒ€í•œ **ê³ í’ˆì§ˆ ì‹œí—˜ ë¬¸ì œ**ë¥¼ ì¶œì œí•©ë‹ˆë‹¤.

**ì •í™•íˆ {qn}ê°œì˜ ë¬¸ì œ**ë¥¼ ìƒì„±í•˜ì„¸ìš”.

{"=" * 70}
ğŸ¯ ì¶œì œ êµ¬ì„±(í•„ìˆ˜)
{"=" * 70}
- ë¬¸ì œ ìœ í˜•: {type_composition}
- ë‚œì´ë„ ë¶„í¬: {diff_composition}
- ëª¨ë“  ë¬¸ì œëŠ” ì•„ë˜ "ê·¼ê±° ì²­í¬ ëª©ë¡"ì— ê¸°ë°˜í•´ì•¼ í•¨
- evidenceëŠ” ê° ë¬¸í•­ë‹¹ 1~2ê°œë§Œ ì‚¬ìš©
- chunk_idëŠ” ì œê³µëœ ëª©ë¡ì— ìˆëŠ” ê²ƒë§Œ ì‚¬ìš©
- ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´
- JSON í˜•ì‹ ì¶œë ¥ (ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ê¸ˆì§€, ì„¤ëª… ë¬¸ì¥ ê¸ˆì§€)

{"=" * 70}
ğŸš« í˜•ì‹/í’ˆì§ˆ ìœ„ë°˜ ì‹œ ì²˜ë¦¬
{"=" * 70}
- JSONì´ ì•„ë‹ˆë©´ ì‹¤íŒ¨
- ì •ë‹µì´ ì• ë§¤í•˜ê±°ë‚˜ ë³µìˆ˜ì •ë‹µ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©´ ì‹¤íŒ¨
- evidenceì™€ ë¬´ê´€í•œ ë¬¸ì œë©´ ì‹¤íŒ¨
- SAQëŠ” 1~5ë‹¨ì–´ "ìš©ì–´/êµ¬"ë¡œë§Œ ë‹µ (ê´„í˜¸ë¡œ ì¥í™©í•œ ì„¤ëª… ê¸ˆì§€)

{quality_block}

{table_instruction}

{"=" * 70}
ì„¹ì…˜ ì „ì²´ ë‚´ìš©(ì›ë¬¸)
{"=" * 70}
{job.get('text', '')}
{table_section}

{"=" * 70}
ê·¼ê±° ì²­í¬ ëª©ë¡(ì—¬ê¸°ì„œë§Œ evidence ì„ íƒ ê°€ëŠ¥)
{"=" * 70}
{chr(10).join(chunk_lines)}

{"=" * 70}
ì¶œë ¥ í˜•ì‹(JSON ONLY)
{"=" * 70}
{output_schema}

ì§€ê¸ˆ ë°”ë¡œ **{qn}ê°œ**ì˜ ë¬¸ì œë¥¼ JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
""".strip()


# =========================
# Postprocess
# =========================

def _ensure_explanation(q: Dict[str, Any]) -> None:
    """explanation ëˆ„ë½ ë°©ì§€"""
    expl = q.get("explanation")
    if isinstance(expl, str) and expl.strip():
        return

    qtype = str(q.get("type") or "").upper()
    ans = q.get("correct_answer") or q.get("answer")

    if qtype == "SAQ":
        if isinstance(ans, str) and ans.strip():
            q["explanation"] = f"ì •ë‹µ: '{ans.strip()}'. ê·¼ê±° ì²­í¬ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤."
        else:
            q["explanation"] = "ê·¼ê±° ì²­í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ë‹µì„ ì‘ì„±í•˜ì„¸ìš”."
    else:
        if isinstance(ans, str) and ans.strip():
            q["explanation"] = f"ì •ë‹µì€ {ans.strip()}ì…ë‹ˆë‹¤. ê·¼ê±° ì²­í¬ì˜ ë‚´ìš©ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤."
        else:
            q["explanation"] = "ê·¼ê±° ì²­í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”."


def _ensure_saq_answer(q: Dict[str, Any]) -> None:
    """SAQ answer ëˆ„ë½ ë°©ì§€ (ë‹¨ë‹µí˜• ê°•ì¡°)"""
    qtype = str(q.get("type") or "").upper()
    if qtype != "SAQ":
        return

    ans = q.get("correct_answer") or q.get("answer")
    if isinstance(ans, str) and ans.strip():
        q["correct_answer"] = ans.strip().split("\n")[0][:100]
        return

    expl = q.get("explanation")
    if isinstance(expl, str) and expl.strip():
        q["correct_answer"] = expl.strip().split("\n")[0][:100]
        return

    q["correct_answer"] = "ê·¼ê±° ì²­í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ìš©ì–´ë¥¼ ë‹¨ë‹µìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."


def _normalize_type(qtype: Any) -> str:
    s = str(qtype or "").strip().upper()
    if s in {"MCQ", "SAQ"}:
        return s
    if s in {"TABLE", "TABULAR"}:
        return "MCQ"
    if s in {"SHORT", "SHORTANSWER"}:
        return "SAQ"
    return "MCQ"


def _normalize_difficulty(d: Any) -> str:
    s = str(d or "").strip().lower()
    if s in {"easy", "medium", "hard"}:
        return s
    return "medium"


def _normalize_question_fields(q: Dict[str, Any]) -> Dict[str, Any]:
    """
    LLM ì¶œë ¥ í•„ë“œëª…ì„ API ëª…ì„¸ í•„ë“œëª…ìœ¼ë¡œ ì •ê·œí™”

    ëª…ì„¸ í•„ë“œëª…:
    - question_text (not question)
    - options (not choices)
    - correct_answer (not answer)
    - source_pages (evidenceì—ì„œ ì¶”ì¶œ)
    - table_refs (generated_tableì´ ìˆìœ¼ë©´ ì¶”ê°€)
    """
    out = dict(q)

    # question_text ì •ê·œí™”
    if "question" in out and "question_text" not in out:
        out["question_text"] = out.pop("question")

    # options ì •ê·œí™”
    if "choices" in out and "options" not in out:
        out["options"] = out.pop("choices")

    # correct_answer ì •ê·œí™”
    if "answer" in out and "correct_answer" not in out:
        out["correct_answer"] = out.pop("answer")

    # source_pages ì¶”ì¶œ (evidenceì—ì„œ)
    if "source_pages" not in out:
        evidence = out.get("evidence", [])
        if isinstance(evidence, list):
            pages = []
            for ev in evidence:
                if isinstance(ev, dict) and isinstance(ev.get("page"), int):
                    pages.append(ev["page"])
            if pages:
                out["source_pages"] = sorted(set(pages))

    # table_refs ìƒì„± (generated_tableì´ ìˆëŠ” ê²½ìš°)
    if "table_refs" not in out and out.get("generated_table"):
        source_pages = out.get("source_pages", [])
        if source_pages:
            out["table_refs"] = [f"table_{source_pages[0]}_1"]
        else:
            out["table_refs"] = ["table_ref"]

    return out


def _extract_json(raw: str) -> Optional[Dict[str, Any]]:
    """JSON ì¶”ì¶œ (markdown ë¸”ë¡ ì œê±°)"""
    if not isinstance(raw, str) or not raw.strip():
        return None
    raw = raw.strip()

    # ```json ... ``` ì œê±°
    if raw.startswith("```"):
        lines = raw.split("\n")
        if lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        raw = "\n".join(lines).strip()

    try:
        obj = json.loads(raw)
        return obj if isinstance(obj, dict) else None
    except Exception:
        pass

    start = raw.find("{")
    end = raw.rfind("}")
    if start != -1 and end != -1 and end > start:
        cand = raw[start:end + 1]
        try:
            obj = json.loads(cand)
            return obj if isinstance(obj, dict) else None
        except Exception:
            return None
    return None


# =========================
# Public API
# =========================

def generate_questions_for_job(
    job: Dict[str, Any],
    cfg: QuestionGenConfig,
    *,
    tables: Optional[List[Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    """
    ë¬¸ì œ ìƒì„± (ê²€ì¦ ì—†ìŒ)
    """
    text = job.get("text") or ""
    chunks = _split_text_to_chunks(text, cfg)

    if not chunks:
        return {
            "questions": [],
            "answers_only": [],
            "error": "NO_EVIDENCE_CHUNKS",
            "raw": "",
            "evidence_candidates": [],
            "meta": {
                "job_id": job.get("job_id"),
                "section_id": job.get("section_id"),
                "num_questions": 0,
                "model": cfg.model,
            },
        }

    if tables is None:
        tables = job.get("tables", [])
    norm_tables = _normalize_tables_format(tables)

    prompt = _build_prompt(job, chunks, tables=norm_tables)

    raw = call_llm_text(
        prompt=prompt,
        model=cfg.model,
        temperature=cfg.temperature,
    )

    data = _extract_json(raw)
    if data is None:
        return {
            "questions": [],
            "answers_only": [],
            "error": "LLM_OUTPUT_NOT_JSON",
            "raw": raw,
            "evidence_candidates": chunks,
            "meta": {
                "job_id": job.get("job_id"),
                "section_id": job.get("section_id"),
                "num_questions": 0,
                "model": cfg.model,
            },
        }

    questions = data.get("questions", [])
    if not isinstance(questions, list):
        questions = []

    # ì •í™•í•œ ê°œìˆ˜ ë§ì¶”ê¸°
    target = int(job.get("target_questions") or 0) or 2
    if len(questions) > target:
        questions = questions[:target]

    normed: List[Dict[str, Any]] = []
    for i, q in enumerate(questions, start=1):
        if not isinstance(q, dict):
            continue

        q = dict(q)
        q["question_id"] = str(q.get("question_id") or f"Q{i:03d}").strip()
        q["type"] = _normalize_type(q.get("type"))
        q["difficulty"] = _normalize_difficulty(q.get("difficulty"))

        # í•„ë“œëª… ì •ê·œí™” (API ëª…ì„¸ ì¤€ìˆ˜)
        q = _normalize_question_fields(q)

        _ensure_saq_answer(q)
        _ensure_explanation(q)

        # í‘œ ì—†ìœ¼ë©´ generated_table, table_refs ì œê±°
        if not norm_tables:
            q.pop("generated_table", None)
            q.pop("table_refs", None)

        normed.append(q)

    answers_only = [
        {
            "question_id": q.get("question_id"),
            "type": q.get("type"),
            "correct_answer": q.get("correct_answer"),
            "explanation": q.get("explanation"),
        }
        for q in normed
    ]

    return {
        "questions": normed,
        "answers_only": answers_only,
        "evidence_candidates": chunks,
        "meta": {
            "job_id": job.get("job_id"),
            "section_id": job.get("section_id"),
            "num_questions": len(normed),
            "model": cfg.model,
            "target_questions": target,
            "actual_questions": len(normed),
        },
    }
