# core/question_orchestrator.py
"""
ë¬¸ì œ ë°°ë¶„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (ìµœì í™” ë²„ì „)

ìµœì í™”:
- Phase 1: LLM ìš”ì•½ â†’ TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (0 API í˜¸ì¶œ)
- Phase 2: LLM ë°°ë¶„ (1íšŒ í˜¸ì¶œ ìœ ì§€)
- Phase 3: í†µê³„ì  ê²€ì¦ (0 API í˜¸ì¶œ)

ê²°ê³¼: Nê°œ ì„¹ì…˜ì—ì„œ N+1 í˜¸ì¶œ â†’ 1íšŒ í˜¸ì¶œ (70%+ ë¹„ìš© ì ˆê°)
"""
from __future__ import annotations

import json
import math
import re
from collections import Counter
from typing import Any, Dict, List, Optional, Set
from pathlib import Path

from core.llm_text import call_llm_text


# =============================================================================
# TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¡œì»¬ ì²˜ë¦¬)
# =============================================================================

# ë¶ˆìš©ì–´ (í•œêµ­ì–´ + ì˜ì–´)
STOPWORDS_KO = {
    "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë°", "ë¥¼", "ì„", "ì˜", "ê°€", "ì—",
    "ëŠ”", "ì€", "ë¡œ", "ìœ¼ë¡œ", "ì—ì„œ", "ê³¼", "ì™€", "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤",
    "ìœ„í•´", "í†µí•´", "ëŒ€í•´", "ëŒ€í•œ", "ê°™ì€", "ë‹¤ë¥¸", "ëª¨ë“ ", "ê°", "í•´ë‹¹",
    "ê²½ìš°", "ë•Œë¬¸", "ë”°ë¼", "ìœ„í•œ", "ìœ„í•˜ì—¬", "ëŒ€í•˜ì—¬", "ê´€í•œ", "ìˆëŠ”",
    "ì—†ëŠ”", "í•˜ëŠ”", "ë˜ëŠ”", "í•˜ì—¬", "ë˜ì–´", "í•œë‹¤", "ëœë‹¤", "í•©ë‹ˆë‹¤",
    "ë©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ë‹ˆë‹¤", "ë‹¤ìŒ", "ì´ë²ˆ", "ì§€ë‚œ", "ì´í›„",
}

STOPWORDS_EN = {
    "the", "a", "an", "is", "are", "was", "were", "be", "been", "being",
    "have", "has", "had", "do", "does", "did", "will", "would", "could",
    "should", "may", "might", "must", "shall", "can", "and", "or", "but",
    "if", "then", "else", "when", "where", "what", "which", "who", "whom",
    "this", "that", "these", "those", "it", "its", "of", "to", "in", "for",
    "on", "with", "at", "by", "from", "as", "into", "through", "during",
    "before", "after", "above", "below", "between", "under", "again",
    "further", "once", "here", "there", "all", "each", "few", "more", "most",
    "other", "some", "such", "no", "not", "only", "own", "same", "so", "than",
    "too", "very", "just", "also", "now", "how", "any", "both", "each",
}

STOPWORDS = STOPWORDS_KO | STOPWORDS_EN


def _tokenize(text: str) -> List[str]:
    """
    ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €
    - ì˜ë¬¸: ì†Œë¬¸ìë¡œ ë¶„ë¦¬
    - í•œê¸€: 2-6ì ë‹¨ì–´ ì¶”ì¶œ
    """
    tokens = []

    # ì˜ë¬¸ í† í°
    en_words = re.findall(r'[a-zA-Z]{2,}', text.lower())
    tokens.extend([w for w in en_words if w not in STOPWORDS_EN and len(w) > 2])

    # í•œê¸€ í† í° (2-6ì)
    ko_words = re.findall(r'[ê°€-í£]{2,6}', text)
    tokens.extend([w for w in ko_words if w not in STOPWORDS_KO])

    return tokens


def _compute_tf(tokens: List[str]) -> Dict[str, float]:
    """Term Frequency ê³„ì‚°"""
    counter = Counter(tokens)
    total = len(tokens) if tokens else 1
    return {word: count / total for word, count in counter.items()}


def _compute_idf(documents: List[List[str]]) -> Dict[str, float]:
    """Inverse Document Frequency ê³„ì‚°"""
    n_docs = len(documents)
    if n_docs == 0:
        return {}

    doc_freq: Dict[str, int] = {}
    for tokens in documents:
        unique_tokens = set(tokens)
        for token in unique_tokens:
            doc_freq[token] = doc_freq.get(token, 0) + 1

    # IDF = log(N / df) + 1
    return {
        word: math.log(n_docs / df) + 1
        for word, df in doc_freq.items()
    }


def _extract_keywords_tfidf(
    text: str,
    idf_scores: Dict[str, float],
    top_k: int = 10,
) -> List[str]:
    """TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    tokens = _tokenize(text)
    tf = _compute_tf(tokens)

    # TF-IDF ì ìˆ˜ ê³„ì‚°
    tfidf_scores = {
        word: tf_score * idf_scores.get(word, 1.0)
        for word, tf_score in tf.items()
    }

    # ìƒìœ„ Kê°œ í‚¤ì›Œë“œ
    sorted_words = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)
    return [word for word, score in sorted_words[:top_k]]


def _analyze_section_local(section: Dict[str, Any], idf_scores: Dict[str, float]) -> Dict[str, Any]:
    """
    ë¡œì»¬ì—ì„œ ì„¹ì…˜ ë¶„ì„ (LLM í˜¸ì¶œ ì—†ìŒ)

    ì¶”ì¶œ ì •ë³´:
    - í‚¤ì›Œë“œ (TF-IDF)
    - í†µê³„ (ê¸¸ì´, í‘œ, ì½”ë“œ, ìˆ˜ì‹)
    """
    section_id = section.get("section_id", "unknown")
    title = section.get("title", "")
    text = section.get("text", "")
    tables = section.get("tables", [])

    # í†µê³„ ì •ë³´
    char_count = len(text)
    has_code = bool(re.search(r'```|def\s+\w+|class\s+\w+|function\s+\w+', text))
    has_math = bool(re.search(r'\$[^$]+\$|\\frac|\\sum|\\int|\\sqrt', text))
    num_tables = len(tables)

    # TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = _extract_keywords_tfidf(text, idf_scores, top_k=8)

    # ì œëª©ì—ì„œë„ í‚¤ì›Œë“œ ì¶”ì¶œ
    title_keywords = _extract_keywords_tfidf(title, idf_scores, top_k=3)

    # í‚¤ì›Œë“œ ë³‘í•© (ì œëª© ìš°ì„ )
    all_keywords = list(dict.fromkeys(title_keywords + keywords))[:10]

    # ìš”ì•½ ìƒì„± (ë¡œì»¬)
    summary = _generate_local_summary(title, all_keywords, has_code, has_math, num_tables)

    return {
        "section_id": section_id,
        "title": title,
        "summary": summary,
        "keywords": all_keywords,
        "stats": {
            "char_count": char_count,
            "num_tables": num_tables,
            "has_code": has_code,
            "has_math": has_math,
        }
    }


def _generate_local_summary(
    title: str,
    keywords: List[str],
    has_code: bool,
    has_math: bool,
    num_tables: int,
) -> str:
    """
    ë¡œì»¬ì—ì„œ ìš”ì•½ ìƒì„± (í…œí”Œë¦¿ ê¸°ë°˜)
    """
    parts = []

    # ì œëª© ê¸°ë°˜
    if title:
        parts.append(title)

    # í‚¤ì›Œë“œ ì¶”ê°€
    if keywords:
        parts.append(f"í•µì‹¬: {', '.join(keywords[:5])}")

    # íŠ¹ì„± ì¶”ê°€
    features = []
    if has_code:
        features.append("ì½”ë“œ í¬í•¨")
    if has_math:
        features.append("ìˆ˜ì‹ í¬í•¨")
    if num_tables > 0:
        features.append(f"í‘œ {num_tables}ê°œ")

    if features:
        parts.append(f"({', '.join(features)})")

    return " ".join(parts)


def _batch_analyze_sections(sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ëª¨ë“  ì„¹ì…˜ ë¶„ì„ (ë³‘ë ¬ ë¶ˆí•„ìš” - ë¡œì»¬ ì²˜ë¦¬ë¼ ë¹ ë¦„)

    1. ì „ì²´ ë¬¸ì„œì—ì„œ IDF ê³„ì‚°
    2. ê° ì„¹ì…˜ë³„ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    # 1. ì „ì²´ ë¬¸ì„œì˜ í† í° ìˆ˜ì§‘ (IDF ê³„ì‚°ìš©)
    all_token_lists = []
    for section in sections:
        text = section.get("text", "")
        tokens = _tokenize(text)
        all_token_lists.append(tokens)

    # 2. IDF ê³„ì‚°
    idf_scores = _compute_idf(all_token_lists)

    # 3. ê° ì„¹ì…˜ ë¶„ì„
    summaries = []
    for section in sections:
        summary = _analyze_section_local(section, idf_scores)
        summaries.append(summary)

    return summaries


# =============================================================================
# Phase 2: í†µí•© íŒë‹¨ (ë‹¨ì¼ LLM í˜¸ì¶œ)
# =============================================================================

def _llm_allocate(summaries: List[Dict[str, Any]], total_questions: int) -> Optional[Dict[str, int]]:
    """
    ìš”ì•½ëœ ì„¹ì…˜ë“¤ì„ LLMì—ê²Œ ë³´ì—¬ì£¼ê³  ë¬¸ì œ ê°œìˆ˜ ë°°ë¶„
    â†’ 1íšŒ í˜¸ì¶œë¡œ ì „ì²´ íŒë‹¨
    """
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    section_lines = []
    for i, s in enumerate(summaries, 1):
        stats = s.get("stats", {})
        keywords = s.get("keywords", [])
        keywords_str = ", ".join(keywords[:5]) if keywords else "(ì—†ìŒ)"

        section_lines.append(
            f"{i}. [{s['section_id']}] {s['title']}\n"
            f"   í‚¤ì›Œë“œ: {keywords_str}\n"
            f"   (ê¸¸ì´: {stats.get('char_count', 0):,}ì, "
            f"í‘œ: {stats.get('num_tables', 0)}ê°œ, "
            f"ì½”ë“œ: {'ìˆìŒ' if stats.get('has_code') else 'ì—†ìŒ'}, "
            f"ìˆ˜ì‹: {'ìˆìŒ' if stats.get('has_math') else 'ì—†ìŒ'})"
        )

    sections_text = "\n\n".join(section_lines)

    max_per = min(15, total_questions // 2) if total_questions > 2 else total_questions

    prompt = f"""ë‹¹ì‹ ì€ êµìœ¡í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°•ì˜ ìë£Œì˜ ì„¹ì…˜ë³„ ì •ë³´ë¥¼ ë³´ê³ , ì´ {total_questions}ê°œì˜ ì‹œí—˜ ë¬¸ì œë¥¼ ê° ì„¹ì…˜ì— ë°°ë¶„í•˜ì„¸ìš”.

{"="*60}
ì„¹ì…˜ ì •ë³´
{"="*60}
{sections_text}

{"="*60}
ë°°ë¶„ ê¸°ì¤€
{"="*60}
1. êµìœ¡ì  ì¤‘ìš”ë„ (í•µì‹¬ ê°œë…, ì´ë¡ )
2. ë‚´ìš© ê¹Šì´ (í…ìŠ¤íŠ¸ ê¸¸ì´)
3. ì‹¤ìŠµ ê°€ëŠ¥ì„± (ì½”ë“œ, í‘œ)
4. ê° ì„¹ì…˜ ìµœì†Œ 1ë¬¸ì œ, ìµœëŒ€ {max_per}ë¬¸ì œ

{"="*60}
ì¶œë ¥ í˜•ì‹ (JSON ONLY)
{"="*60}
{{
  "allocation": {{
    "ì„¹ì…˜ID": ë¬¸ì œê°œìˆ˜,
    ...
  }},
  "reasoning": "ë°°ë¶„ ê·¼ê±°ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ"
}}

ì§€ê¸ˆ ë°”ë¡œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

    try:
        response = call_llm_text(
            prompt=prompt,
            model="gpt-4o-mini",
            temperature=0.3,
        )

        # JSON ì¶”ì¶œ
        response = response.strip()
        if response.startswith("```json"):
            response = response[7:]
        if response.startswith("```"):
            response = response[3:]
        if response.endswith("```"):
            response = response[:-3]
        response = response.strip()

        data = json.loads(response)
        allocation = data.get("allocation", {})
        reasoning = data.get("reasoning", "")

        # ê²€ì¦
        if not isinstance(allocation, dict):
            return None

        # section_id ë§¤í•‘ í™•ì¸
        result = {}
        for s in summaries:
            sid = s["section_id"]
            # ë‹¤ì–‘í•œ key ì‹œë„
            count = allocation.get(sid) or allocation.get(f"[{sid}]") or allocation.get(s["title"])
            if isinstance(count, (int, float)):
                result[sid] = int(count)

        if not result:
            return None

        print(f"âœ… LLM ë°°ë¶„ ê·¼ê±°: {reasoning}")
        return result

    except Exception as e:
        print(f"âš ï¸ LLM ë°°ë¶„ ì‹¤íŒ¨: {e}")
        return None


# =============================================================================
# Phase 3: í†µê³„ì  Fallback
# =============================================================================

def _statistical_allocate(summaries: List[Dict[str, Any]], total_questions: int) -> Dict[str, int]:
    """
    í†µê³„ì  ë°©ë²• (fallback)
    â†’ LLM ì‹¤íŒ¨ ì‹œ ì‚¬ìš©
    - ìš”ì²­ ìˆ˜ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ë„ë¡ ë³´ì¥
    """
    if not summaries or total_questions <= 0:
        return {}

    weights = []
    for s in summaries:
        stats = s.get("stats", {})
        char_count = stats.get("char_count", 0)
        num_tables = stats.get("num_tables", 0)
        has_code = stats.get("has_code", False)
        has_math = stats.get("has_math", False)

        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        w = math.sqrt(max(char_count, 1))
        w += num_tables * 50
        w += 30 if has_code else 0
        w += 20 if has_math else 0
        weights.append(max(w, 1.0))

    total_weight = sum(weights)

    # âœ… ë¹„ìœ¨ ê¸°ë°˜ ì´ˆê¸° ë°°ë¶„ (ìµœì†Œê°’ 0 í—ˆìš©)
    allocation = {}
    for s, w in zip(summaries, weights):
        count = int(total_questions * w / total_weight)  # floor
        allocation[s["section_id"]] = count

    # âœ… í•©ê³„ ì •í™•íˆ ë§ì¶”ê¸°
    current_total = sum(allocation.values())
    diff = total_questions - current_total

    # ê°€ì¤‘ì¹˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_sections = sorted(summaries, key=lambda x: weights[summaries.index(x)], reverse=True)

    if diff > 0:
        # ë¶€ì¡±í•˜ë©´ ê°€ì¤‘ì¹˜ ë†’ì€ ì„¹ì…˜ë¶€í„° ì¶”ê°€
        for i in range(diff):
            sid = sorted_sections[i % len(sorted_sections)]["section_id"]
            allocation[sid] += 1
    elif diff < 0:
        # ì´ˆê³¼í•˜ë©´ ê°€ì¤‘ì¹˜ ë‚®ì€ ì„¹ì…˜ë¶€í„° ê°ì†Œ
        sorted_sections_asc = list(reversed(sorted_sections))
        for i in range(abs(diff)):
            sid = sorted_sections_asc[i % len(sorted_sections_asc)]["section_id"]
            if allocation[sid] > 0:
                allocation[sid] -= 1

    # âœ… 0ì¸ ì„¹ì…˜ ì œê±°
    allocation = {sid: count for sid, count in allocation.items() if count > 0}

    return allocation


def _validate_allocation(
    allocation: Dict[str, int],
    summaries: List[Dict[str, Any]],
    total_questions: int,
    min_per_section: int = 0,  # âœ… ê¸°ë³¸ê°’ 0ìœ¼ë¡œ ë³€ê²½
    max_per_section: int = 15,
) -> Dict[str, int]:
    """
    LLM ë°°ë¶„ ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
    - ìš”ì²­ ìˆ˜ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ë„ë¡ ë³´ì •
    """
    # ë²”ìœ„ ì œí•œ (maxë§Œ ì ìš©, minì€ í•©ê³„ ì¡°ì • í›„ ì ìš©)
    for sid in allocation:
        allocation[sid] = min(max_per_section, max(0, allocation[sid]))

    # í•©ê³„ ë§ì¶”ê¸° (ì •í™•íˆ total_questionsê°€ ë˜ë„ë¡)
    current = sum(allocation.values())
    diff = total_questions - current

    if diff != 0:
        # í†µê³„ì  ê°€ì¤‘ì¹˜ë¡œ ë³´ì •
        stats_alloc = _statistical_allocate(summaries, total_questions)
        sorted_sids = sorted(stats_alloc.keys(), key=lambda x: stats_alloc[x], reverse=True)

        iterations = 0
        max_iterations = abs(diff) * len(sorted_sids) + 100  # ë¬´í•œë£¨í”„ ë°©ì§€

        while diff != 0 and iterations < max_iterations:
            changed = False
            for sid in sorted_sids:
                if diff == 0:
                    break
                if sid in allocation:
                    if diff > 0 and allocation[sid] < max_per_section:
                        allocation[sid] += 1
                        diff -= 1
                        changed = True
                    elif diff < 0 and allocation[sid] > 0:  # âœ… min_per_section ëŒ€ì‹  0
                        allocation[sid] -= 1
                        diff += 1
                        changed = True
            if not changed:
                break
            iterations += 1

    # âœ… 0ì¸ ì„¹ì…˜ ì œê±° (Job Builderì—ì„œ ë¶ˆí•„ìš”í•œ Job ìƒì„± ë°©ì§€)
    allocation = {sid: count for sid, count in allocation.items() if count > 0}

    return allocation


# =============================================================================
# Public API
# =============================================================================

def orchestrate_question_allocation(
    sections: List[Dict[str, Any]],
    total_questions: int,
    use_llm: bool = True,
    max_workers: int = 4,  # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ (ì‚¬ìš© ì•ˆí•¨)
    cache_dir: Optional[Path] = None,
) -> Dict[str, Any]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ Orchestration (ìµœì í™” ë²„ì „)

    ë³€ê²½ì‚¬í•­:
    - Phase 1: LLM ìš”ì•½ â†’ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ (ë¡œì»¬)
    - Phase 2: LLM ë°°ë¶„ (1íšŒ í˜¸ì¶œ ìœ ì§€)
    - Phase 3: í†µê³„ì  ê²€ì¦ (ë¡œì»¬)

    Args:
        sections: ì„¹ì…˜ ë°ì´í„° (text, tables í¬í•¨)
        total_questions: ìƒì„±í•  ì´ ë¬¸ì œ ê°œìˆ˜
        use_llm: LLM ì‚¬ìš© ì—¬ë¶€
        max_workers: (ì‚¬ìš© ì•ˆí•¨, í•˜ìœ„ í˜¸í™˜ì„±)
        cache_dir: ë¶„ì„ ìºì‹œ ë””ë ‰í† ë¦¬

    Returns:
        {
            "allocation": {"ì„¹ì…˜ID": ë¬¸ì œê°œìˆ˜, ...},
            "method": "llm" | "statistical",
            "summaries": [...],
            "total": 15
        }
    """
    print(f"ğŸ¯ Orchestrator ì‹œì‘: {len(sections)}ê°œ ì„¹ì…˜, {total_questions}ê°œ ë¬¸ì œ")

    # ìºì‹œ í™•ì¸
    summaries = None
    if cache_dir and (cache_dir / "summaries.json").exists():
        try:
            print("âœ… ìºì‹œëœ ë¶„ì„ ì‚¬ìš©")
            summaries = json.loads((cache_dir / "summaries.json").read_text(encoding="utf-8"))
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
            summaries = None

    # Phase 1: ë¡œì»¬ ë¶„ì„ (TF-IDF ê¸°ë°˜)
    if summaries is None:
        print("ğŸ“ Phase 1: ì„¹ì…˜ ë¶„ì„ ì¤‘ (TF-IDF)...")
        summaries = _batch_analyze_sections(sections)
        print(f"âœ… {len(summaries)}ê°œ ì„¹ì…˜ ë¶„ì„ ì™„ë£Œ (0 API í˜¸ì¶œ)")

        # ìºì‹±
        if cache_dir:
            try:
                cache_dir.mkdir(parents=True, exist_ok=True)
                (cache_dir / "summaries.json").write_text(
                    json.dumps(summaries, ensure_ascii=False, indent=2),
                    encoding="utf-8"
                )
                print(f"ğŸ’¾ ë¶„ì„ ìºì‹œ ì €ì¥: {cache_dir / 'summaries.json'}")
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # Phase 2: LLM íŒë‹¨ (ì„ íƒì )
    allocation = None
    method = "statistical"

    if use_llm:
        print("ğŸ¤– Phase 2: LLM ê¸°ë°˜ ë°°ë¶„ ì¤‘ (1íšŒ í˜¸ì¶œ)...")
        allocation = _llm_allocate(summaries, total_questions)

        if allocation:
            print("âœ… LLM ë°°ë¶„ ì„±ê³µ")
            method = "llm"

            # Phase 3: ê²€ì¦ ë° ë³´ì •
            print("ğŸ” Phase 3: ê²€ì¦ ë° ë³´ì • ì¤‘...")
            allocation = _validate_allocation(
                allocation,
                summaries,
                total_questions,
                min_per_section=1,
                max_per_section=min(15, max(3, total_questions // 2)),
            )
        else:
            print("âš ï¸ LLM ë°°ë¶„ ì‹¤íŒ¨ â†’ í†µê³„ì  ë°©ë²• ì‚¬ìš©")

    # Fallback: í†µê³„ì  ë°©ë²•
    if allocation is None:
        print("ğŸ“Š í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ë°°ë¶„ ì¤‘...")
        allocation = _statistical_allocate(summaries, total_questions)

    print(f"âœ… ìµœì¢… ë°°ë¶„: {allocation}")

    return {
        "allocation": allocation,
        "method": method,
        "summaries": summaries,
        "total": sum(allocation.values()),
    }
