# backend/engine/cli/run_job.py
"""
ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
"""
import subprocess
import sys
import json
from pathlib import Path
from dotenv import load_dotenv
load_dotenv(override=True)


def run_step(name, cmd, job_store=None, job_id=None, stage=None, step_num=0, total_steps=0):
    """ë‹¨ê³„ ì‹¤í–‰ ë° ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
    print(f"\n{'='*60}")
    print(f"â–¶ {name}")
    print(f"{'='*60}")

    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (ì‹œì‘)
    if job_store and job_id and stage:
        try:
            job_store.update_job_progress(job_id, stage, step_num, total_steps)
        except Exception as e:
            print(f"âš ï¸  ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    result = subprocess.run(cmd, shell=True)
    if result.returncode != 0:
        print(f"âŒ {name} ì‹¤íŒ¨!")
        # ì‹¤íŒ¨ ì‹œ Job ìƒíƒœ ì—…ë°ì´íŠ¸
        if job_store and job_id:
            try:
                job_store.fail_job(job_id, "STEP_FAILED", f"{name} ë‹¨ê³„ ì‹¤íŒ¨")
            except:
                pass
        sys.exit(1)
    print(f"âœ… {name} ì™„ë£Œ")


def run_orchestrator(out_dir: Path, total_questions: int, use_llm: bool = True) -> dict:
    """
    Orchestrator ì‹¤í–‰ â†’ allocation ë°˜í™˜
    """
    from backend.engine.core.question_orchestrator import orchestrate_question_allocation

    # section_contextsì—ì„œ ì„¹ì…˜ ì •ë³´ ì½ê¸°
    contexts_dir = out_dir / "section_contexts"
    index_path = contexts_dir / "index.json"

    if not index_path.exists():
        print("âš ï¸ section_contexts/index.json ì—†ìŒ")
        return {}

    index_data = json.loads(index_path.read_text(encoding="utf-8"))

    sections = []
    for item in index_data.get("items", []):
        # pathê°€ "section_contexts\\section_S001.json" í˜•íƒœì¼ ìˆ˜ ìˆìŒ
        item_path = item["path"]
        # ìƒëŒ€ ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
        if "\\" in item_path or "/" in item_path:
            item_path = Path(item_path).name
        section_path = contexts_dir / item_path
        if section_path.exists():
            section_data = json.loads(section_path.read_text(encoding="utf-8"))
            sections.append(section_data)

    if not sections:
        print("âš ï¸ ì„¹ì…˜ ë°ì´í„° ì—†ìŒ")
        return {}

    # Orchestrator ì‹¤í–‰
    result = orchestrate_question_allocation(
        sections=sections,
        total_questions=total_questions,
        use_llm=use_llm,
        max_workers=4,
        cache_dir=out_dir / "cache",
    )

    allocation = result["allocation"]
    method = result["method"]

    print(f"âœ… Orchestration ì™„ë£Œ ({method} ë°©ì‹)")
    print(f"   ë°°ë¶„: {allocation}")

    # âœ… allocation ì €ì¥ (Job Builderê°€ ì½ì„ ìˆ˜ ìˆë„ë¡)
    allocation_path = out_dir / "allocation.json"
    allocation_path.parent.mkdir(parents=True, exist_ok=True)
    allocation_path.write_text(
        json.dumps(allocation, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    print(f"ğŸ’¾ Allocation ì €ì¥: {allocation_path}")

    return allocation


def convert_to_spec(pdf_id: str, out_dir: Path):
    """
    ëª…ì„¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    input_path = out_dir / "questions_verified_aggregate.json"
    output_path = Path("data") / "results" / f"{pdf_id}.questions.json"

    if not input_path.exists():
        print(f"âš ï¸ ì…ë ¥ íŒŒì¼ ì—†ìŒ: {input_path}")
        return 0, output_path

    data = json.loads(input_path.read_text(encoding="utf-8"))

    questions = []
    q_counter = 1

    for item in data.get("items", []):
        for q in item.get("questions", []):
            if q.get("verdict") != "OK":
                continue

            converted = {
                "question_id": f"q_{q_counter:03d}",
                "type": q["type"],
                "difficulty": q["difficulty"],
                "verdict": q["verdict"],
                "question_text": q.get("question_text") or q.get("question", ""),
                "correct_answer": q.get("correct_answer") or q.get("answer", ""),
                "explanation": q.get("explanation", "")
            }

            if q["type"] == "MCQ":
                options = []
                for choice in q.get("options") or q.get("choices", []):
                    text = choice.split(") ", 1)[1] if ") " in choice else choice
                    options.append(text)
                converted["options"] = options

            # ëª…ì„¸ í•„ë“œ ì¶”ê°€
            if "source_pages" in q:
                converted["source_pages"] = q["source_pages"]
            if "table_refs" in q:
                converted["table_refs"] = q["table_refs"]
            if "confidence" in q:
                converted["confidence"] = q["confidence"]
            if "issues" in q:
                converted["issues"] = q["issues"]

            if "generated_table" in q and q["generated_table"]:
                converted["generated_table"] = q["generated_table"]

            if "evidence" in q and q["evidence"]:
                converted["evidence"] = q["evidence"]

            questions.append(converted)
            q_counter += 1

    result = {
        "pdf_id": pdf_id,
        "total_questions": len(questions),
        "questions": questions
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(
        json.dumps(result, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    return len(questions), output_path


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--pdf_id", required=True, help="PDF ID")
    parser.add_argument("--job_id", required=True, help="Job ID")
    parser.add_argument("--num_questions", type=int, default=10, help="ìƒì„±í•  ì´ ë¬¸ì œ ê°œìˆ˜")
    parser.add_argument("--difficulty", type=str, default="mixed",
                        choices=["easy", "medium", "hard", "mixed"], help="ë¬¸ì œ ë‚œì´ë„")
    parser.add_argument("--mcq_ratio", type=float, default=1.0, help="MCQ(ê°ê´€ì‹) ë¹„ìœ¨ (0.0~1.0)")
    parser.add_argument("--saq_ratio", type=float, default=0.0, help="SAQ(ë‹¨ë‹µí˜•) ë¹„ìœ¨ (0.0~1.0)")
    parser.add_argument("--no_llm_orchestrate", action="store_true", help="LLM ì‚¬ìš© ì•ˆí•¨ (í†µê³„ì  ë°©ë²•)")
    args = parser.parse_args()

    base = Path(__file__).parent.parent.parent  # backend/ ë””ë ‰í† ë¦¬
    pdf_path = str(base / "data" / "pdfs" / f"{args.pdf_id}.pdf")
    pdf_id = args.pdf_id
    job_id = args.job_id
    out_dir = base / "artifacts" / pdf_id

    # sys.path ì„¤ì • (JobStore importìš©)
    import sys
    sys.path.insert(0, str(base))
    from app.storage.job_store import JobStore

    try:
        total_steps = 9  # ì „ì²´ ë‹¨ê³„ ìˆ˜ (orchestrator í¬í•¨)

        # 1. Prepare (PARSING)
        run_step("1. PDF ì¤€ë¹„",
            f'python -m core.prepare_runner --pdf_path "{pdf_path}" --pdf_id {pdf_id} --out_dir "{out_dir}"',
            job_store=JobStore, job_id=job_id, stage="PARSING", step_num=1, total_steps=total_steps)

        # 2. Table Presence (PARSING)
        run_step("2. í‘œ ì¡´ì¬ í™•ì¸",
            f'python -m core.run_table_presence --out_dir "{out_dir}" --pdf_id {pdf_id}',
            job_store=JobStore, job_id=job_id, stage="PARSING", step_num=2, total_steps=total_steps)

        # 3. Table Extract (PARSING)
        run_step("3. í‘œ ì¶”ì¶œ",
            f'python -m core.run_table_extract_mm --out_dir "{out_dir}" --pdf_id {pdf_id}',
            job_store=JobStore, job_id=job_id, stage="PARSING", step_num=3, total_steps=total_steps)

        # 4. Section Indexer (PARSING)
        run_step("4. ì„¹ì…˜ ì¸ë±ì‹±",
            f'python -m core.section_indexer --out_dir "{out_dir}" --pdf_id {pdf_id}',
            job_store=JobStore, job_id=job_id, stage="PARSING", step_num=4, total_steps=total_steps)

        # 5. Section Packager (PARSING)
        run_step("5. ì„¹ì…˜ íŒ¨í‚¤ì§•",
            f'python -m core.section_context_packager --out_dir "{out_dir}" --pdf_id {pdf_id}',
            job_store=JobStore, job_id=job_id, stage="PARSING", step_num=5, total_steps=total_steps)

        # 5.5. Orchestrator (ë¬¸ì œ ìˆ˜ ë°°ë¶„)
        print("\n" + "="*60)
        print("â–¶ 5.5. ë¬¸ì œ ê°œìˆ˜ ë°°ë¶„ (Orchestrator)")
        print("="*60)
        JobStore.update_job_progress(job_id, "PARSING", 6, total_steps)
        allocation = run_orchestrator(
            out_dir=out_dir,
            total_questions=args.num_questions,
            use_llm=not args.no_llm_orchestrate
        )

        if not allocation:
            raise Exception("Orchestrator ì‹¤íŒ¨: allocation ì—†ìŒ")

        # 6. Job Builder (PARSING)
        run_step("6. Job ë¹Œë“œ",
            f'python -m core.job_builder --out_dir "{out_dir}" --pdf_id {pdf_id} '
            f'--allocation_file "{out_dir}/allocation.json" --overwrite '
            f'--difficulty {args.difficulty} --mcq_ratio {args.mcq_ratio} --saq_ratio {args.saq_ratio}',
            job_store=JobStore, job_id=job_id, stage="PARSING", step_num=7, total_steps=total_steps)

        # 7. Question Pipeline (GENERATING/VERIFYING)
        JobStore.update_job_progress(job_id, "GENERATING", 8, total_steps)
        run_step("7. ë¬¸ì œ ìƒì„±",
            f'python -m core.run_question_pipeline --out_dir "{out_dir}" --pdf_id {pdf_id}',
            job_store=None, job_id=None, stage=None, step_num=0, total_steps=0)  # run_question_pipeline ë‚´ë¶€ì—ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸

        # 8. API ëª…ì„¸ í˜•ì‹ ë³€í™˜ (SAVING)
        print("\n" + "="*60)
        print("â–¶ 8. API ëª…ì„¸ í˜•ì‹ ë³€í™˜")
        print("="*60)
        JobStore.update_job_progress(job_id, "SAVING", 9, total_steps)
        q_count, output_path = convert_to_spec(pdf_id, out_dir)
        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {q_count}ê°œ ë¬¸ì œ")

        # Job ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        JobStore.complete_job(job_id)
        print(f"âœ… Job ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {job_id}")

        print("\n" + "="*60)
        print("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ğŸ“ ì›ë³¸: {out_dir}/questions_verified_aggregate.json")
        print(f"ğŸ“ ëª…ì„¸: {output_path}")
        print(f"ğŸ“Š ìš”ì²­: {args.num_questions}ê°œ / ìƒì„±: {q_count}ê°œ")
        print("="*60)
    except Exception as e:
        # Job ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
        import sys
        sys.path.insert(0, str(base))
        from app.storage.job_store import JobStore
        try:
            JobStore.fail_job(job_id, "PIPELINE_ERROR", str(e))
            print(f"âŒ Job ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸: {job_id}")
        except:
            pass
        raise
